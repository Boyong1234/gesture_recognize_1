import os
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import data_load_explore


def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    tf_data_root = 'D:/python3/anaconda/PythonMyFirstWork/datasetHere'
    os.environ['TFDS_DATA_DIR'] = os.path.join(tf_data_root, 'datasets')
    os.makedirs(os.environ['TFDS_DATA_DIR'], exist_ok=True)

    model_dir = os.path.join(tf_data_root, 'models')
    os.makedirs(model_dir, exist_ok=True)

    return tf_data_root, model_dir


def load_and_preprocess():
    (ds_train, ds_test), ds_info = tfds.load(
        'rock_paper_scissors',
        split=['train', 'test'],
        shuffle_files=True,
        as_supervised=True,
        with_info=True,
    )

    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
    print(f"   è®­ç»ƒé›†: {ds_info.splits['train'].num_examples} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {ds_info.splits['test'].num_examples} æ ·æœ¬")
    print(f"   ç±»åˆ«: {ds_info.features['label'].names}")

    def preprocess_image(image, label):
        """é¢„å¤„ç†å•å¼ å›¾ç‰‡"""
        # è°ƒæ•´å¤§å°åˆ° 150x150ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
        image = tf.image.resize(image, [150, 150])
        # å½’ä¸€åŒ–åˆ° [0, 1]
        image = tf.cast(image, tf.float32) / 255.0
        return image, label

    # åº”ç”¨é¢„å¤„ç†
    ds_train = ds_train.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    ds_test = ds_test.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)

    # æ‰¹é‡å¤„ç†å’Œä¼˜åŒ–
    batch_size = 32
    ds_train = ds_train.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
    print(f"   æ‰¹é‡å¤§å°: {batch_size}")
    print(f"   å›¾åƒå°ºå¯¸: 150x150")

    return ds_train, ds_test, ds_info


def create_cnn_model():
    """åˆ›å»ºCNNæ¨¡å‹"""
    print("ğŸ¤– æ„å»ºCNNæ¨¡å‹...")

    model = tf.keras.Sequential([
        # ç¬¬ä¸€å·ç§¯å—
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
        tf.keras.layers.MaxPooling2D(2, 2),

        # ç¬¬äºŒå·ç§¯å—
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),

        # ç¬¬ä¸‰å·ç§¯å—
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),

        # å±•å¹³å±‚
        tf.keras.layers.Flatten(),

        # å…¨è¿æ¥å±‚
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.5),  # é˜²æ­¢è¿‡æ‹Ÿåˆ

        # è¾“å‡ºå±‚
        tf.keras.layers.Dense(3, activation='softmax')  # 3ä¸ªç±»åˆ«
    ])

    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print("âœ… æ¨¡å‹æ„å»ºå®Œæˆ!")
    model.summary()

    return model


def train_model(model, train_data, test_data, model_dir):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹...")

    # è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(model_dir, 'best_model.h5'),
            save_best_only=True,
            monitor='val_accuracy',
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=3,
            min_lr=0.0001,
            verbose=1
        )
    ]

    # å¼€å§‹è®­ç»ƒ
    history = model.fit(
        train_data,
        epochs=20,
        validation_data=test_data,
        callbacks=callbacks,
        verbose=1
    )

    return history


def evaluate_model(model, test_data, ds_info):
    """è¯„ä¼°æ¨¡å‹"""
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

    # è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡
    test_loss, test_accuracy = model.evaluate(test_data, verbose=0)
    print(f"ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.2%}")
    print(f"ğŸ“‰ æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")

    # ç”Ÿæˆé¢„æµ‹
    y_true = []
    y_pred = []

    for images, labels in test_data:
        predictions = model.predict(images, verbose=0)
        y_true.extend(labels.numpy())
        y_pred.extend(np.argmax(predictions, axis=1))

    # åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred,
                                target_names=ds_info.features['label'].names))

    # æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=ds_info.features['label'].names,
                yticklabels=ds_info.features['label'].names)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.show()

    def plot_training_history(history):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒå†å²...")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # å‡†ç¡®ç‡æ›²çº¿
        ax1.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
        ax1.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡')
        ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax1.set_ylabel('å‡†ç¡®ç‡')
        ax1.legend()
        ax1.grid(True)

        # æŸå¤±æ›²çº¿
        ax2.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
        ax2.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
        ax2.set_title('æ¨¡å‹æŸå¤±')
        ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax2.set_ylabel('æŸå¤±')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

        def main():
            """ä¸»å‡½æ•°"""
            print("ğŸš€ å¼€å§‹å®Œæ•´çš„CNNæ‰‹åŠ¿è¯†åˆ«é¡¹ç›®")
            print("=" * 50)

            # 1. è®¾ç½®ç¯å¢ƒ
            tf_data_root, model_dir = setup_environment()

            # 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
            ds_train, ds_test, ds_info = data_load_explore()

            # 3. åˆ›å»ºæ¨¡å‹
            model = create_cnn_model()

            # 4. è®­ç»ƒæ¨¡å‹
            history = train_model(model, ds_train, ds_test, model_dir)

            # 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = os.path.join(model_dir, 'final_gesture_model.h5')
            model.save(final_model_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_model_path}")

            # 6. è¯„ä¼°æ¨¡å‹
            evaluate_model(model, ds_test, ds_info)

            # 7. ç»˜åˆ¶è®­ç»ƒå†å²
            plot_training_history(history)

            print("\nğŸ‰ é¡¹ç›®å®Œæˆ! ä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œå®æ—¶æ‰‹åŠ¿è¯†åˆ«ã€‚")

        if __name__ == "__main__":
            main()